{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File for training word2vec model over a large sentence dataset, importing of paypal data and removing stopwords\n",
    "#importing important libraries. \n",
    "\n",
    "# importing modules needed for comments' cleanup and model training\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordPunctTokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import contractions\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import re, string, unicodedata\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing paypal dataset and cleaning it for english feedbacks only.\n",
    "\n",
    "feedback_data = './LandingPage.csv'\n",
    "data = pd.read_csv(feedback_data)\n",
    "\n",
    "#Assuming data structure is same for all the feedbacks from different domains\n",
    "\n",
    "data = data[['Comments', 'Overall Rating', 'countryCode', 'languageCode',\n",
    "              'Device Type', 'Browser Name', 'IP Address', 'Submission Date',\n",
    "              'session_replay', 'Email Address', 'tealeafId', 'guid', 'custId']]\n",
    "\n",
    "language = 'en'\n",
    "feedback_data = data.loc[data['languageCode'] == language]\n",
    "feedback_data = feedback_data.loc[data['Comments'].notnull() == True]\n",
    "\n",
    "stopwords = './stopwords.csv'\n",
    "stopset = np.array(pd.read_csv(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of feedback comments\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    text = re.sub('\\([^]]*\\)', '', text)\n",
    "    text = re.sub('\\[[^]]*\\]', '', text)\n",
    "    text = re.sub('\\\"[^]]*\\\"', '', text)\n",
    "    return text\n",
    "    \n",
    "def replace_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def doc_clean(feedback):\n",
    "    feedback_tokens = WordPunctTokenizer().tokenize(feedback)\n",
    "    clean = feedback_tokens\n",
    "    clean = [token.lower() for token in feedback_tokens if token.lower() not in stopset and len(token) > 2 and token.isalpha()]\n",
    "    for i in range(len(clean)):\n",
    "        clean[i] = unicode(clean[i],'utf-8')\n",
    "    return clean\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def preprocessing(comment):\n",
    "    temp = remove_between_square_brackets(comment)\n",
    "    temp = replace_contractions(temp)\n",
    "    temp = doc_clean(temp)\n",
    "    temp = remove_non_ascii(temp)\n",
    "    temp = remove_punctuation(temp)\n",
    "    temp = lemmatize_verbs(temp)\n",
    "    return temp\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9646\n"
     ]
    }
   ],
   "source": [
    "# Preparing data and training word2vec\n",
    "#appending paypal feedbacks to dataset\n",
    "raw_comments = np.array(feedback_data[['Comments', 'Overall Rating', 'IP Address', 'Submission Date'\n",
    "                                       , 'session_replay', 'tealeafId', 'guid', 'custId']])\n",
    "\n",
    "clean_comments = []\n",
    "unclean_comments = []\n",
    "for i in range(len(raw_comments)):\n",
    "    processed = preprocessing(raw_comments[i][0])\n",
    "    if(len(processed)>0):\n",
    "        clean_comments.append(processed)\n",
    "        unclean_comments.append(raw_comments[i])\n",
    "        \n",
    "        \n",
    "print(len(clean_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving processed comments\n",
    "\n",
    "feedback_data.to_csv(\"feedback_data_en.csv\")\n",
    "\n",
    "with open('processed_comments', 'wb') as f:\n",
    "    pickle.dump(clean_comments, f)\n",
    "    \n",
    "with open('unprocessed_comments', 'wb') as f:\n",
    "    pickle.dump(unclean_comments, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run if you have huge corpus of feedbacks else use google's word2vec\n",
    "\n",
    "train = clean_comments\n",
    "model = gensim.models.Word2Vec (train, size=300, window=12, min_count=1, workers=10)\n",
    "model.train(train,total_examples=len(train),epochs=60)\n",
    "model.save(\"word2vec_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking how well our model is trained\n",
    "\n",
    "check_word = 'error'\n",
    "w2 = [check_word]\n",
    "model.wv.most_similar (w2,topn=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
